{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\nerf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import imageio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from main import config_parser\n",
    "from data_loader.load_llff import load_llff_data\n",
    "from model import create_nerf, img2mse, mse2psnr\n",
    "from render import get_rays_np, render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = config_parser()\n",
    "args = parser.parse_args(args='--config configs/ckc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image data (292, 135, 3, 95) [292.         135.         202.50087625]\n",
      "Loaded ./data/nerf_llff_data/ckc 2.6607503791705396 105.38726428465726\n",
      "Data:\n",
      "(95, 3, 5) (95, 292, 135, 3) (95, 2)\n",
      "HOLDOUT view is 41\n",
      "Loaded llff (95, 292, 135, 3) (120, 3, 5) [292.      135.      202.50087] ./data/nerf_llff_data/ckc\n",
      "Auto LLFF holdout, 8\n",
      "DEFINING BOUNDS\n",
      "NEAR FAR 0.1832474336028099 8.064537048339844\n"
     ]
    }
   ],
   "source": [
    "K = None\n",
    "device = torch.device(\"cpu\")\n",
    "images, poses, bds, render_poses, i_test = load_llff_data(args.datadir, args.factor,\n",
    "                                                          recenter=True, bd_factor=.75,\n",
    "                                                          spherify=args.spherify)\n",
    "hwf = poses[0,:3,-1]\n",
    "poses = poses[:,:3,:4]\n",
    "print('Loaded llff', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "if not isinstance(i_test, list):\n",
    "    i_test = [i_test]\n",
    "if args.llffhold > 0:                   # take every 1/N images as LLFF test set,\n",
    "    print('Auto LLFF holdout,', args.llffhold)\n",
    "    i_test = np.arange(images.shape[0])[::args.llffhold]\n",
    "i_val = i_test\n",
    "i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n",
    "                (i not in i_test and i not in i_val)])\n",
    "print('DEFINING BOUNDS')\n",
    "if args.no_ndc:\n",
    "    near = np.ndarray.min(bds) * .9\n",
    "    far = np.ndarray.max(bds) * 1.\n",
    "    \n",
    "else:\n",
    "    near = 0.\n",
    "    far = 1.\n",
    "print('NEAR FAR', near, far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts []\n",
      "Not ndc!\n"
     ]
    }
   ],
   "source": [
    "H, W, focal = hwf\n",
    "H, W = int(H), int(W)\n",
    "hwf = [H, W, focal]\n",
    "if K is None:\n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5*W],\n",
    "        [0, focal, 0.5*H],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "# Create nerf model\n",
    "render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args, device)\n",
    "global_step = start         # ckpt number \n",
    "bds_dict = {\n",
    "    'near' : near,\n",
    "    'far' : far,\n",
    "}\n",
    "render_kwargs_train.update(bds_dict)            # insert near and far values\n",
    "render_kwargs_test.update(bds_dict)\n",
    "# Move testing data to GPU\n",
    "render_poses = torch.Tensor(render_poses).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get rays\n",
      "done, concats\n",
      "shuffle rays\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "N_rand = args.N_rand\n",
    "print('get rays')\n",
    "rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0)   # [N, ro+rd, H, W, 3]\n",
    "print('done, concats')\n",
    "rays_rgb = np.concatenate([rays, images[:,None]], 1)                    # [N, ro+rd+rgb, H, W, 3]\n",
    "rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4])                          # [N, H, W, ro+rd+rgb, 3]\n",
    "rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0)                  # train images only\n",
    "rays_rgb = np.reshape(rays_rgb, [-1,3,3])                               # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "rays_rgb = rays_rgb.astype(np.float32)\n",
    "print('shuffle rays')\n",
    "np.random.shuffle(rays_rgb)\n",
    "print('done')\n",
    "i_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.Tensor(images).to(device)\n",
    "poses = torch.Tensor(poses).to(device)\n",
    "rays_rgb = torch.Tensor(rays_rgb).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m     rays_rgb \u001b[38;5;241m=\u001b[39m rays_rgb[rand_idx]\n\u001b[0;32m     15\u001b[0m     i_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 18\u001b[0m rgb, disp, acc, extras \u001b[38;5;241m=\u001b[39m render(H, W, K, chunk\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mchunk, rays\u001b[38;5;241m=\u001b[39mbatch_rays,\n\u001b[0;32m     19\u001b[0m                                         verbose\u001b[38;5;241m=\u001b[39mi \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m, retraw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m                                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrender_kwargs_train)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m img_loss \u001b[38;5;241m=\u001b[39m img2mse(rgb, target_s)\n",
      "File \u001b[1;32md:\\Works\\my-nerf\\render.py:363\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(H, W, K, chunk, rays, c2w, ndc, near, far, use_viewdirs, c2w_staticcam, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m     rays \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rays, viewdirs], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Render and reshape\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m all_ret \u001b[38;5;241m=\u001b[39m batchify_rays(rays, chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_ret:\n\u001b[0;32m    365\u001b[0m     k_sh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sh[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_ret[k]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32md:\\Works\\my-nerf\\render.py:294\u001b[0m, in \u001b[0;36mbatchify_rays\u001b[1;34m(rays_flat, chunk, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m all_ret \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, rays_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], chunk):\n\u001b[1;32m--> 294\u001b[0m     ret \u001b[38;5;241m=\u001b[39m render_rays(rays_flat[i:i\u001b[38;5;241m+\u001b[39mchunk], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ret:\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_ret:\n",
      "File \u001b[1;32md:\\Works\\my-nerf\\render.py:214\u001b[0m, in \u001b[0;36mrender_rays\u001b[1;34m(ray_batch, network_fn, network_query_fn, N_samples, retraw, lindisp, perturb, N_importance, network_fine, white_bkgd, raw_noise_std, verbose, pytest)\u001b[0m\n\u001b[0;32m    212\u001b[0m t_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m, steps\u001b[38;5;241m=\u001b[39mN_samples)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lindisp:\n\u001b[1;32m--> 214\u001b[0m     z_vals \u001b[38;5;241m=\u001b[39m \u001b[43mnear\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mt_vals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m far \u001b[38;5;241m*\u001b[39m (t_vals)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     z_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mnear \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m-\u001b[39mt_vals) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mfar \u001b[38;5;241m*\u001b[39m (t_vals))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "for i in range(1, 25):\n",
    "    print(i)\n",
    "    time0 = time.time()\n",
    "    # Sample random ray batch\n",
    "    # Random over all images\n",
    "    batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n",
    "    batch = torch.transpose(batch, 0, 1)\n",
    "    batch_rays, target_s = batch[:2], batch[2]\n",
    "    i_batch += N_rand\n",
    "    if i_batch >= rays_rgb.shape[0]:\n",
    "        print(\"Shuffle data after an epoch!\")\n",
    "        rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "        rays_rgb = rays_rgb[rand_idx]\n",
    "        i_batch = 0\n",
    "            \n",
    "\n",
    "    rgb, disp, acc, extras = render(H, W, K, chunk=args.chunk, rays=batch_rays,\n",
    "                                            verbose=i < 10, retraw=True,\n",
    "                                            **render_kwargs_train)\n",
    "    optimizer.zero_grad()\n",
    "    img_loss = img2mse(rgb, target_s)\n",
    "    trans = extras['raw'][...,-1]\n",
    "    loss = img_loss\n",
    "    psnr = mse2psnr(img_loss)\n",
    "    if 'rgb0' in extras:\n",
    "        img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "        loss = loss + img_loss0\n",
    "        psnr0 = mse2psnr(img_loss0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # NOTE: IMPORTANT!\n",
    "    ###   update learning rate   ###\n",
    "    decay_rate = 0.1\n",
    "    decay_steps = args.lrate_decay * 1000\n",
    "    new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "    ################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
